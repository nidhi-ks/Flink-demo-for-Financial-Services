Please follow these steps to get started with the workshop:

Create a free account:

Sign up at Confluent Cloud at https://www.confluent.io/confluent-cloud/tryfree/.

Create a new environment:

After logging in, click on 'Add new environment' and give it a meaningful name (e.g., <your name>-environment).
For now, you can select the Essentials package for Stream Governance.
Set up a new cluster:

Click on 'Add cluster' and select a basic cluster for the workshop.
Choose your preferred cloud provider and region, then click Launch.
And that’s it! Your Confluent Cloud Kafka cluster is now up and running!


Creating a datagen connector to pull data in Confluent Cloud : 

Follow these steps to create a topic and ingest sample data:

Create a new topic:

Navigate to the Topics tab and click Create New Topic.
Name the topic 'trades-data'.
Add a sample data connector:

Go to the Connectors tab, click Add Connector, and search for the Sample Data connector.
Select 'trades-data' as the target topic. Select output message format to be avro
Configure the connector:

In the Advanced Configuration section, choose Trades as the data set.
Leave the other settings as default and click to start the connector provisioning.
After a few seconds, the connector will start running, and you can verify that data is being ingested into the 'trades-data' topic.

Create a new topic:

Navigate to the Topics tab and click Create New Topic.
Name the topic 'users-data'.
Add a sample data connector:

Go to the Connectors tab, click Add Connector, and search for the Sample Data connector.
Select 'users-data' as the target topic.Select output message format to be avro
Configure the connector:

In the Advanced Configuration section, choose Users as the data set.
Leave the other settings as default and click to start the connector provisioning.
After a few seconds, the connector will start running, and you can verify that data is being ingested into the 'users-data' topic.

Creating a compute pool in Flink : 

Follow these steps to set up a compute pool in Flink:

Navigate to the Flink tab:

Hover over the Flink tab and click on Create Compute Pool.
Configure the compute pool:

Ensure the region you select matches the region of your Kafka cluster.
Choose your preferred cloud provider and region.
You can leave the Max CFU setting as default.
Provide a meaningful name for your compute pool.
It will take a couple of minutes, and your compute pool will be up and running.

Creating tables in Flink : 

1. To review the trade_data table created by flink run SHOW CREATE TABLE trade_data;

Output : CREATE TABLE `Nidhi-workspace`.`flinkdemo-finserv`.`trade_data` (
  `key` VARBINARY(2147483647),
  `side` VARCHAR(2147483647) NOT NULL COMMENT 'A simulated trade side (buy or sell or short)',
  `quantity` INT NOT NULL COMMENT 'A simulated random quantity of the trade',
  `symbol` VARCHAR(2147483647) NOT NULL COMMENT 'Simulated stock symbols',
  `price` INT NOT NULL COMMENT 'A simulated random trade price in pennies',
  `account` VARCHAR(2147483647) NOT NULL COMMENT 'Simulated accounts assigned to the trade',
  `userid` VARCHAR(2147483647) NOT NULL COMMENT 'The simulated user who executed the trade'
) DISTRIBUTED BY HASH(`key`) INTO 3 BUCKETS
WITH (
  'changelog.mode' = 'append',
  'connector' = 'confluent',
  'kafka.cleanup-policy' = 'delete',
  'kafka.max-message-size' = '2097164 bytes',
  'kafka.retention.size' = '0 bytes',
  'kafka.retention.time' = '7 d',
  'key.format' = 'raw',
  'scan.bounded.mode' = 'unbounded',
  'scan.startup.mode' = 'earliest-offset',
  'value.format' = 'avro-registry'
);

2. Let's add all the data from trade_data table to trades_topic , please run the below command 

CREATE TABLE `Nidhi-workspace`.`flinkdemo-finserv`.`trades_topic` (
  `key` VARBINARY(2147483647),
  `side` VARCHAR(2147483647) NOT NULL COMMENT 'A simulated trade side (buy or sell or short)',
  `quantity` INT NOT NULL COMMENT 'A simulated random quantity of the trade',
  `symbol` VARCHAR(2147483647) NOT NULL COMMENT 'Simulated stock symbols',
  `price` INT NOT NULL COMMENT 'A simulated random trade price in pennies',
  `account` VARCHAR(2147483647) NOT NULL COMMENT 'Simulated accounts assigned to the trade',
  `userid` VARCHAR(2147483647) NOT NULL COMMENT 'The simulated user who executed the trade'
) ;

insert into trades_topic select * from trade_data;

Filtering in Flink : 

Filtering the trade data which has price and quantity greater than 0 .

Create a table filtered_trades

CREATE TABLE `Nidhi-workspace`.`flinkdemo-finserv`.`filtered_trades` (
  `key` VARBINARY(2147483647),
  `side` VARCHAR(2147483647) NOT NULL COMMENT 'A simulated trade side (buy or sell or short)',
  `quantity` INT NOT NULL COMMENT 'A simulated random quantity of the trade',
  `symbol` VARCHAR(2147483647) NOT NULL COMMENT 'Simulated stock symbols',
  `price` INT NOT NULL COMMENT 'A simulated random trade price in pennies',
  `account` VARCHAR(2147483647) NOT NULL COMMENT 'Simulated accounts assigned to the trade',
  `userid` VARCHAR(2147483647) NOT NULL COMMENT 'The simulated user who executed the trade'
) ;

Insert the data by filtering 

INSERT INTO filtered_trades
SELECT *
FROM trades_topic
WHERE quantity > 0
  AND price > 0
  AND (side = 'BUY' OR side = 'SELL');

Joining two tables users data and trades data in Flink using inner join :

SELECT 
    t.side,
    t.quantity,
    t.symbol,
    t.price,
    t.account,
    t.userid,
    u.registertime,
    u.regionid,
    u.gender
FROM 
    trades_topic t
INNER JOIN 
    users_data u 
ON 
    t.userid = u.userid;


Running Aggregates and Window functions 

We will run a tumbling window function which calaculates the total volume of the shares traded and sum of the price of each user for every 10 minutes 

Creating the table broker_trade_volume 

CREATE TABLE broker_trade_volume (
     window_start TIMESTAMP(3),
    window_end TIMESTAMP(3),
  userid STRING,
    total_number_of_shares BIGINT,
  total_amount_traded bigint
  
);

Running aggregate with windowing 

Insert into broker_trade_volume
SELECT window_start, window_end, userid , SUM(quantity) as `total_number_of_shares`, sum(price) as total_amount_traded
  FROM TABLE(
    TUMBLE(TABLE filtered_trades, DESCRIPTOR($rowtime), INTERVAL '10' minutes))
  GROUP BY userid , window_start, window_end;

The output of the above query is inserted into broker_trade_volume topic . I want to push the data into MongoDB for storing . 

Prerequisite : Have a Mongo Atlas cluster  running 

Creating Mongo Atlas Sink connector : 

Follow these steps to set up the MongoDB Atlas Sink connector:

Navigate to the Connectors tab:

Hover over the Connectors tab and select the MongoDB Atlas Sink Connector.
Select the topic:

Choose the topic you want to push data from. In this case, it’s the broker_trade_volume topic.
Provide authentication details:

Enter the required details such as hostname, database username, password, database name, and collection name.
Configuration and validation:

The system will internally validate the connector's connection to MongoDB Atlas. You can leave the remaining settings as default.
After a few seconds, your MongoDB connector will be running, and you can verify the data ingestion into your MongoDB Atlas cluster.

Below is the connector configuration for you to verify 

Configuration : 
{
  "config": {
    "connector.class": "MongoDbAtlasSink",
    "name": "MongoDbAtlasSinkConnector_1",
    "schema.context.name": "default",
    "input.data.format": "AVRO",
    "cdc.handler": "None",
    "value.subject.name.strategy": "TopicNameStrategy",
    "delete.on.null.values": "false",
    "max.batch.size": "0",
    "bulk.write.ordered": "true",
    "rate.limiting.timeout": "0",
    "rate.limiting.every.n": "0",
    "write.strategy": "DefaultWriteModelStrategy",
    "kafka.auth.mode": "KAFKA_API_KEY",
    "kafka.api.key": "MNQKPKTJSJUAX3DH",
    "kafka.api.secret": "****************************************************************",
    "topics": "broker_trade_volume",
    "connection.host": "cflt-test.5afyk.mongodb.net",
    "connection.user": "test-cflt",
    "connection.password": "*********",
    "database": "trade-data",
    "collection": "trade_data_volume",
    "doc.id.strategy": "BsonOidStrategy",
    "doc.id.strategy.overwrite.existing": "false",
    "document.id.strategy.uuid.format": "string",
    "key.projection.type": "none",
    "value.projection.type": "none",
    "namespace.mapper.class": "DefaultNamespaceMapper",
    "server.api.deprecation.errors": "false",
    "server.api.strict": "false",
    "max.num.retries": "3",
    "retries.defer.timeout": "5000",
    "timeseries.timefield.auto.convert": "false",
    "timeseries.timefield.auto.convert.date.format": "yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]",
    "timeseries.timefield.auto.convert.locale.language.tag": "en",
    "timeseries.expire.after.seconds": "0",
    "ts.granularity": "None",
    "max.poll.interval.ms": "300000",
    "max.poll.records": "500",
    "tasks.max": "1"
  }
}

Please note that the user should have read write access on the database , collection specified in the connector . The data flows from Confluent Cloud to Mongo Atlas trade_data_volume collection . 
Verify the data inside the Mongo Atlas UI . 
